{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-31 16:27:01.992368: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-31 16:27:02.044760: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-31 16:27:03.026109: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# set current file as the working directory\n",
    "# os.chdir(os.path.dirname(os.path.abspath(__file__)))\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# from model import Model\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import argparse\n",
    "import pickle\n",
    "# from helper.stats import stats_dataset\n",
    "# from helper.utils import pickle_store_dataset,load_from_pickle\n",
    "import time \n",
    "import tensorflow as tf\n",
    "import concurrent.futures\n",
    "import torch\n",
    "import sys\n",
    "from vit_pytorch import SimpleViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data'\n",
    "def load_pickle(file_name):\n",
    "    with open(f'{data_dir}/{file_name}', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# Use concurrent.futures to load the data in parallel\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=70) as executor:\n",
    "    train_data_future = executor.submit(load_pickle, 'train_data.pkl')\n",
    "    train_labels_future = executor.submit(load_pickle, 'train_labels.pkl')\n",
    "    # test_data_future = executor.submit(load_pickle, 'test_data.pkl')\n",
    "    # test_labels_future = executor.submit(load_pickle, 'test_labels.pkl')\n",
    "    val_data_future = executor.submit(load_pickle, 'val_data.pkl')\n",
    "    val_labels_future = executor.submit(load_pickle, 'val_labels.pkl')\n",
    "\n",
    "    # Wait for all futures to complete\n",
    "    train_data = train_data_future.result()\n",
    "    train_labels = train_labels_future.result()\n",
    "    # test_data = test_data_future.result()\n",
    "    # test_labels = test_labels_future.result()\n",
    "    val_data = val_data_future.result()\n",
    "    val_labels = val_labels_future.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleViT(\n",
       "  (to_patch_embedding): Sequential(\n",
       "    (0): Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=128, p2=128)\n",
       "    (1): LayerNorm((49152,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): Linear(in_features=49152, out_features=32, bias=True)\n",
       "    (3): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x ModuleList(\n",
       "        (0): Attention(\n",
       "          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (attend): Softmax(dim=-1)\n",
       "          (to_qkv): Linear(in_features=32, out_features=3072, bias=False)\n",
       "          (to_out): Linear(in_features=1024, out_features=32, bias=False)\n",
       "        )\n",
       "        (1): FeedForward(\n",
       "          (net): Sequential(\n",
       "            (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=32, out_features=2048, bias=True)\n",
       "            (2): GELU(approximate='none')\n",
       "            (3): Linear(in_features=2048, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (to_latent): Identity()\n",
       "  (linear_head): Linear(in_features=32, out_features=43, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = SimpleViT(\n",
    "    image_size = 256,\n",
    "    patch_size = 128,\n",
    "    num_classes = 43,\n",
    "    dim = 32,\n",
    "    depth = 6,\n",
    "    heads = 16,\n",
    "    mlp_dim = 2048\n",
    ")\n",
    "\n",
    "\n",
    "v.train(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.transpose(train_data, (0, 3, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61201, 3, 256, 256)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_tensor = tf.convert_to_tensor(train_data)\n",
    "train_tensor = torch.from_numpy(train_data)\n",
    "train_tensor = train_tensor.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([61201, 3, 256, 256])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tensor.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "v(train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
